{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "817cfd59-a54b-4394-9656-9d0e79711d0c",
   "metadata": {},
   "source": [
    "### **Load Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dc69a03-0055-4ae8-91d9-52ebb078f872",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.4\n",
      "1.4.39\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy as sa\n",
    "\n",
    "print(pd.__version__)\n",
    "print(sa.__version__)\n",
    "\n",
    "from config import oltp_conn_string, warehouse_conn_string, oltp_tables, warehouse_tables, dimension_columns, ddl_statements, ddl_marts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f36d37-dd56-4c5f-8690-dd88bc6f510f",
   "metadata": {},
   "source": [
    "### **Function ETL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ff56c09-846c-49c5-84c3-1491ffd98082",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_tables():\n",
    "    \"\"\"Create tables in the data warehouse if they do not exist.\"\"\"\n",
    "    engine = sa.create_engine(warehouse_conn_string)\n",
    "    with engine.connect() as conn:\n",
    "        for ddl in ddl_statements.values():\n",
    "            conn.execute(ddl)\n",
    "            \n",
    "def extract_data(table_name):\n",
    "    \"\"\"Extract data from a table in the OLTP database.\"\"\"\n",
    "    engine = sa.create_engine(oltp_conn_string)\n",
    "    query = f\"SELECT * FROM {oltp_tables[table_name]}\"\n",
    "    df = pd.read_sql(query, engine)\n",
    "    print(f'Extract Data {oltp_tables[table_name]} Success')\n",
    "    return df\n",
    "\n",
    "def transform_data(df, target_table):\n",
    "    \"\"\"Transform the extracted data to match the schema of the target dimension table.\"\"\"\n",
    "    columns = dimension_columns.get(target_table)\n",
    "    if columns:\n",
    "        df = df[columns]\n",
    "    print(f'Transform Data {target_table} Success')\n",
    "    return df\n",
    "\n",
    "def transform_fact_orders():\n",
    "    \"\"\"Transform data for the fact_orders table.\"\"\"\n",
    "    dataframes = {table: extract_data(table) for table in oltp_tables.keys()}\n",
    "\n",
    "    df_orders = dataframes['orders']\n",
    "    df_orders = df_orders.merge(dataframes['users'], on='user_id')\n",
    "    df_orders = df_orders.merge(dataframes['payments'], on='payment_id')\n",
    "    df_orders = df_orders.merge(dataframes['shippers'], on='shipper_id')\n",
    "    df_orders = df_orders.merge(dataframes['ratings'], on='rating_id')\n",
    "    df_orders = df_orders.merge(dataframes['vouchers'], how='left', on='voucher_id')\n",
    "    df_orders.rename(columns={'user_id_x': 'user_id'}, inplace=True)\n",
    "    \n",
    "    fact_orders_columns = dimension_columns.get('fact_orders')\n",
    "    return df_orders[fact_orders_columns]\n",
    "\n",
    "def transform_fact_orders_items():\n",
    "    \"\"\"Transform data for the fact_orders_items table.\"\"\"\n",
    "    dataframes = {table: extract_data(table) for table in oltp_tables.keys()}\n",
    "\n",
    "    df_orders_items = dataframes['orders_items']\n",
    "    df_orders_items = df_orders_items.merge(dataframes['orders'], on='order_id')\n",
    "    df_orders_items = df_orders_items.merge(dataframes['products'], on='product_id')\n",
    "    df_orders_items = df_orders_items.merge(dataframes['products'], on='product_discount')\n",
    "    df_orders_items = df_orders_items.merge(dataframes['products'], on='product_price')    \n",
    "    df_orders_items.rename(columns={'order_id_x': 'order_id'}, inplace=True)\n",
    "    \n",
    "    fact_orders_items_columns = dimension_columns.get('fact_orders_items')\n",
    "    return df_orders_items[fact_orders_items_columns]\n",
    "\n",
    "def load_data(df, table_name):\n",
    "    \"\"\"Load the transformed data into the target table in the data warehouse.\"\"\"\n",
    "    engine = sa.create_engine(warehouse_conn_string)\n",
    "    with engine.connect() as conn:\n",
    "        # Cek kunci unique\n",
    "        unique_key = get_unique_key(table_name)  # Misalnya user_id untuk tabel dim_user\n",
    "        existing_data = pd.read_sql(f\"SELECT {unique_key} FROM {table_name}\", conn)\n",
    "        \n",
    "        # Deduplikasi data\n",
    "        df = deduplicate_data(df, existing_data, unique_key)\n",
    "        \n",
    "        # Masukkan data baru\n",
    "        df.to_sql(table_name, conn, index=False, if_exists='append', method='multi')\n",
    "        print(f'Load Data {table_name} Success')\n",
    "        \n",
    "def deduplicate_data(new_data, existing_data, unique_key):\n",
    "    \"\"\"Remove duplicates from new data based on existing data.\"\"\"\n",
    "    existing_keys = existing_data[unique_key].tolist()\n",
    "    unique_rows = new_data[~new_data[unique_key].isin(existing_keys)]\n",
    "    return unique_rows\n",
    "\n",
    "def get_unique_key(table_name):\n",
    "    \"\"\"Retrieve the unique key of the table.\"\"\"\n",
    "    if table_name == 'dim_user':\n",
    "        return 'user_id'\n",
    "    elif table_name == 'dim_payment':\n",
    "        return 'payment_id'\n",
    "    elif table_name == 'dim_shipper':\n",
    "        return 'shipper_id'\n",
    "    elif table_name == 'dim_rating':\n",
    "        return 'rating_id'\n",
    "    elif table_name == 'dim_voucher':\n",
    "        return 'voucher_id'\n",
    "    elif table_name == 'fact_orders':\n",
    "        return 'order_id'\n",
    "    elif table_name == 'dim_product_category':\n",
    "        return 'product_category_id'\n",
    "    elif table_name == 'dim_product':\n",
    "        return 'product_id'\n",
    "    elif table_name == 'fact_orders_items':\n",
    "        return 'order_item_id'\n",
    "    # Tambahkan kondisi lain jika ada tabel lain    \n",
    "    else:\n",
    "        raise ValueError(\"Table name not recognized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a0dab5-2f9d-472d-8350-ad46d2e7e60a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Function Data Mart**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d93de0d-ea8e-422d-aa68-1c1132427704",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_and_insert_dm_sales():\n",
    "    \n",
    "    \"\"\"Create dm_sales table and insert data into it.\"\"\"\n",
    "    engine = sa.create_engine(warehouse_conn_string)\n",
    "    with engine.connect() as conn:\n",
    "        # Fetch existing data from dm_sales table\n",
    "        existing_data = pd.read_sql('SELECT * FROM dm_sales', conn)\n",
    "        # Create dm_sales table\n",
    "        conn.execute(ddl_marts['dim_sales'])\n",
    "\n",
    "        # Insert data into dm_sales table\n",
    "        conn.execute(ddl_marts['insert_dm_sales'])\n",
    "\n",
    "\n",
    "    # Fetch newly inserted data\n",
    "        new_data = pd.read_sql('SELECT * FROM dm_sales', conn)        \n",
    "        \n",
    "        # Deduplicate data\n",
    "        unique_key = ['order_id', 'product_id']\n",
    "        deduplicated_data = deduplicate_data(new_data, existing_data, unique_key)\n",
    "        \n",
    "        # Insert deduplicated data into dm_sales table\n",
    "        deduplicated_data.to_sql('dm_sales', conn, if_exists='append', index=False)\n",
    "    print(f'Data Mart Has Create Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4afa360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_insert_dm_sales():\n",
    "    \"\"\"Create dm_sales table and insert data into it.\"\"\"\n",
    "    engine = sa.create_engine(warehouse_conn_string)\n",
    "    with engine.connect() as conn:\n",
    "        # Create dm_sales table\n",
    "        conn.execute(ddl_marts['dim_sales'])\n",
    "        \n",
    "        # Fetch existing data from dm_sales table if it exists\n",
    "        try:\n",
    "            existing_data = pd.read_sql('SELECT * FROM dm_sales', conn)\n",
    "        except sa.exc.ProgrammingError:\n",
    "            # Handle case where dm_sales table doesn't exist yet\n",
    "            existing_data = pd.DataFrame()\n",
    "        \n",
    "        # Insert data into dm_sales table\n",
    "        conn.execute(ddl_marts['insert_dm_sales'])\n",
    "        \n",
    "        # Fetch newly inserted data\n",
    "        new_data = pd.read_sql('SELECT * FROM dm_sales', conn)\n",
    "        \n",
    "        # Deduplicate data        \n",
    "        deduplicated_data = deduplicate_data(new_data, existing_data, 'order_id')\n",
    "        \n",
    "        # Insert deduplicated data into dm_sales table\n",
    "        deduplicated_data.to_sql('dm_sales', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    print(f'Data Mart Has Create Success')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09cd1cd-e9ac-4a99-91f2-f3b9dcc28312",
   "metadata": {},
   "source": [
    "### **Function Run**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10330d98-0373-42aa-8307-caa46e6bf0ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def etl_process():\n",
    "    \"\"\"Run the entire ETL process.\"\"\"\n",
    "    # Create tables\n",
    "    create_tables()\n",
    "\n",
    "    # Process dimension tables\n",
    "    for dim_table, target_table in warehouse_tables.items():\n",
    "        if dim_table != 'fact_orders':\n",
    "            source_table = dim_table\n",
    "            df = extract_data(source_table)\n",
    "            transformed_df = transform_data(df, dim_table)\n",
    "            load_data(transformed_df, target_table)\n",
    "        else:\n",
    "            # Process fact table\n",
    "            df_fact_orders = transform_fact_orders()\n",
    "            load_data(df_fact_orders, target_table)\n",
    "\n",
    "    # proses mart table\n",
    "    create_and_insert_dm_sales()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f468a-fb5f-476f-a7d0-5cca7d859a96",
   "metadata": {},
   "source": [
    "# **Run ETL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2738ed9d-6512-4f11-b7be-400976529462",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract Data tb_users Success\n",
      "Transform Data users Success\n",
      "Load Data dim_user Success\n",
      "Extract Data tb_payments Success\n",
      "Transform Data payments Success\n",
      "Load Data dim_payment Success\n",
      "Extract Data tb_shippers Success\n",
      "Transform Data shippers Success\n",
      "Load Data dim_shipper Success\n",
      "Extract Data tb_ratings Success\n",
      "Transform Data ratings Success\n",
      "Load Data dim_rating Success\n",
      "Extract Data tb_vouchers Success\n",
      "Transform Data vouchers Success\n",
      "Load Data dim_voucher Success\n",
      "Extract Data tb_orders Success\n",
      "Transform Data orders Success\n",
      "Load Data fact_orders Success\n",
      "Extract Data tb_product_category Success\n",
      "Transform Data product_category Success\n",
      "Load Data dim_product_category Success\n",
      "Extract Data tb_products Success\n",
      "Transform Data products Success\n",
      "Load Data dim_product Success\n",
      "Extract Data tb_order_items Success\n",
      "Transform Data orders_items Success\n",
      "Load Data fact_orders_items Success\n",
      "Data Mart Has Create Success\n"
     ]
    }
   ],
   "source": [
    "#script running all ETL\n",
    "etl_process()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1e1fec-86cb-4426-bc7b-28b5ab27efec",
   "metadata": {},
   "source": [
    "### **Run Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba73e31-b2fe-478c-bad1-b756ad924fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_tables()\n",
    "\n",
    "source_table = 'users'\n",
    "df = extract_data(source_table)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af6dfee-7af4-412b-9dc4-537ec3689526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformed_df = transform_data(df, 'dim_user')\n",
    "transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c95c03-3427-4248-a2ef-1dee7f556f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_data(transformed_df, 'dim_user')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63afee92-a119-4e39-9afd-5aec4b238e1b",
   "metadata": {},
   "source": [
    "### **Script Upload Google Sheets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gspread\n",
    "%pip install oauth2client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86ce8176-06b3-46b7-9328-679fd00545bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "with open('digitalskola_key.json','rb') as file:\n",
    "    key = json.load(file)\n",
    "    \n",
    "scope = ['https://www.googleapis.com/auth/drive','https://spreadsheets.google.com/feeds']\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_dict(key, scope)\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "###tambahkan email googledigitalskola@digitalskola-368401.iam.gserviceaccount.com ke dalam google sheet anda#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155dce82-f692-450f-81e7-720013c3a3d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fetch_data_from_dwh(query):\n",
    "     # Membuat koneksi ke database\n",
    "    engine = sa.create_engine(warehouse_conn_string)\n",
    "    \n",
    "    # Membuat hasil query menjadi Dataframe\n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_mart = fetch_data_from_dwh(\"\"\"SELECT * FROM dm_sales;\"\"\")\n",
    "df_mart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71476a8b-dde7-4794-93da-04c25d3faa0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ganti dengan nama google sheets anda\n",
    "sheet = client.open('Datamart')\n",
    "\n",
    "# ganti sesuai dengan nama sheet didalam google sheets anda\n",
    "# siapkan nama kolom pada sheet di google sheet anda\n",
    "\n",
    "export = sheet.worksheet('Sheet1')\n",
    "export.update([df_mart.columns.values.tolist()] + df_mart.astype(str).values.tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
